{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tensorflow.contrib import rnn\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.reset_default_graph()\n",
    "from IPython.core.display import Image, display\n",
    "from collections import deque\n",
    "import gym\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "- https://ai.intel.com/demystifying-deep-reinforcement-learning/\n",
    "- https://github.com/simoninithomas/Deep_reinforcement_learning_Course\n",
    "- https://courses.cs.washington.edu/courses/csep573/12au/lectures/18-rl.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning based on Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "- https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "\n",
    "- http://karpathy.github.io/2016/05/31/rl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False}, # for testing purpose, set it to be deterministic\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78, # optimum = .8196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Action size is: ' + str(action_size))\n",
    "print('State size is: ' + str(state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Table:   $f(State, Action) ->  Q\\ Value$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_episodes = 50000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Q Table Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    # reset evn\n",
    "    state = env.reset() # set state --> 0, state from {0,1,2,...,15} \n",
    "    step = 0\n",
    "    done = False\n",
    "    rewards_episode = 0\n",
    "    \n",
    "    # start this episode\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # decide action\n",
    "        random_0_1 = random.uniform(0, 1)\n",
    "        if random_0_1 > epsilon: # exploitation\n",
    "            action = np.argmax(qtable[state,:]) # select max q action given state\n",
    "            \n",
    "        else: # exploration\n",
    "            action = env.action_space.sample() # action sampled from {0,1,2,3}\n",
    "            #LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
    "        \n",
    "        # implement action\n",
    "        new_state, reward, done, info = env.step(action) # reward from {0,1}, done from {True, False}\n",
    "        \n",
    "        # update q table\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "            learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "            \n",
    "        # update reward and state\n",
    "        rewards_episode += reward\n",
    "        state = new_state\n",
    "\n",
    "        # Do not continue if done\n",
    "        if done == True: \n",
    "            break\n",
    "            \n",
    "    # After finishing one episode\n",
    "    episode += 1\n",
    "    \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(rewards_episode)\n",
    "\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://randomant.net/images/algorithm-behind-curtain-2/q_learning_algorithm.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for episode in range(5):\n",
    "    \n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "       \n",
    "        env.render()\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning based on deep learning NN\n",
    "Ref:\n",
    "\n",
    "- https://medium.com/@joshpatterson_5192/introduction-to-deep-q-learning-1bded90a6193\n",
    "- https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8\n",
    "\n",
    "- https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_reinforcement_learning.html\n",
    "\n",
    "- http://slazebni.cs.illinois.edu/spring17/lec17_rl.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "observation = env.reset()\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#env.render()\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess to reduce space\n",
    "cppied from https://github.com/dhruvp/atari-pong/blob/master/me_pong.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downsample(image):\n",
    "    # Take only alternate pixels - basically halves the resolution of the image (which is fine for us)\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_color(image):\n",
    "    \"\"\"Convert all color (RGB is the third dimension in the image)\"\"\"\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_background(image):\n",
    "    image[image == 144] = 0\n",
    "    image[image == 109] = 0\n",
    "    return image\n",
    "\n",
    "def preprocess_state(input_observation):\n",
    "    \"\"\" convert the 210x160x3 uint8 frame into a 6400 float vector \"\"\"\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = downsample(processed_observation)\n",
    "    processed_observation = remove_color(processed_observation)\n",
    "    processed_observation = remove_background(processed_observation)\n",
    "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    \n",
    "    # Convert from 80 x 80 matrix to 1600 x 1 matrix\n",
    "    processed_observation = processed_observation.astype(np.float64).ravel()\n",
    "    return processed_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_state(observation).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic\n",
    "total_episodes =         # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 2000            # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Memory\n",
    "memory_size = 50000\n",
    "batch_size = 10000\n",
    "pretrain_length = batch_size\n",
    "possible_actions = [2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define memory\n",
    "https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]\n",
    "    def all_records(self):\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Render the environment\n",
    "init_state = env.reset()\n",
    "\n",
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    #env.render()\n",
    "    if i == 0:\n",
    "        state = init_state\n",
    "        \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "    else:\n",
    "        next_state = next_state\n",
    "        \n",
    "    memory.add((preprocess_state(state), action, reward, preprocess_state(next_state), done))\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.sample(1) # state, action, reward, next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how random action performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_loses = 0\n",
    "num_wins = 0\n",
    "for record in memory.all_records():\n",
    "    if record[2] < 0:\n",
    "        num_loses += 1\n",
    "    if record[2] > 0:\n",
    "        num_wins +=1\n",
    "print('Num of loses: ', num_loses, '; Num of wins:', num_wins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define NN (for now, simplest 1-hidden layer NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q_learning_NN(features, labels, mode):\n",
    "    \n",
    "    # Read action\n",
    "    actions = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        actions = features['actions']\n",
    "    \n",
    "    # input state\n",
    "    input_layer = features[\"states\"] # - 6400\n",
    "    #input_layer = tf.layers.flatten(input_layer) # - 6400\n",
    "    \n",
    "    # hidden layer\n",
    "    num_hidden_units = 200\n",
    "    hidden = tf.layers.dense(inputs = input_layer , \n",
    "                            units = num_hidden_units,\n",
    "                            activation = tf.nn.relu)\n",
    "\n",
    "    # FC layer (dense layer)\n",
    "    logits = tf.layers.dense(inputs = hidden, units = 2)# move up or down\n",
    "    \n",
    "    Final_EstimatorSpec = GenerateEstimatorSpec_Q(logits, labels, mode, actions)\n",
    "    return(Final_EstimatorSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GenerateEstimatorSpec_Q(logits, targets, mode, actions):\n",
    "    # Generate Predictions\n",
    "    predictions = {\n",
    "      \"best_action_index\":  tf.argmax(input = logits, axis=1, name = \"V_1\"),\n",
    "      \"predicted_Q\": logits\n",
    "    }\n",
    "    \n",
    "    # If during PREDICTION mode, just return the predictions\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, \n",
    "                                          predictions = predictions)\n",
    "    \n",
    "    # If during TRAIN, calculate squared loss\n",
    "    predicted_Qs = tf.reduce_sum(tf.multiply(logits, actions), axis=1) # e.g., [q1, q2] * [1.0, 0.0] for 1st action\n",
    "    loss = tf.reduce_mean(tf.square(targets - predicted_Qs))\n",
    "\n",
    "    # If during TRAIN, update gradients\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, \n",
    "                                          loss = loss, \n",
    "                                          train_op = train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/1760/1*ZqML2CCqo455qkxlxJTT2Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"best_action_index\": \"V_1\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors = tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_classifier = tf.estimator.Estimator(\n",
    "    model_fn = Q_learning_NN, # model function type, \n",
    "    model_dir = \"./model_files_3\",\n",
    "    params = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(states, Q_classifier):\n",
    "    pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x = {'states': states}, #features \n",
    "        y = None,\n",
    "        shuffle = False)\n",
    "    \n",
    "    best_action_index = []\n",
    "    predicted_maxQ = []\n",
    "    \n",
    "    pred_results = Q_classifier.predict(input_fn = pred_input_fn)\n",
    "    for pred in pred_results:\n",
    "        best_action_index.append(pred['best_action_index'])\n",
    "        predicted_maxQ.append(np.max(pred['predicted_Q']))\n",
    "    \n",
    "    return {'best_action_index': best_action_index, \n",
    "            'predicted_maxQ': predicted_maxQ}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_step = 0 # explore/exploit trade-off\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    # initialize game\n",
    "    state = preprocess_state(env.reset())\n",
    "    step = 0\n",
    "    done = False\n",
    "    reward = 0\n",
    "    rewards_episode = 0\n",
    "        \n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    while step < max_steps:\n",
    "        \n",
    "        # increase decay\n",
    "        env.render()\n",
    "        decay_step +=1\n",
    "        step += 1\n",
    "        \n",
    "        # decide action\n",
    "        random_0_1 = random.uniform(0, 1)\n",
    "        explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "        \n",
    "        if random_0_1 <= explore_probability or (episode == 0): # exploration\n",
    "            action = random.choice(possible_actions) # action sampled from {2,3}\n",
    "            \n",
    "        else: # exploration\n",
    "            best_action_index = get_prediction(np.array([state], np.float64), Q_classifier)['best_action_index'][0]\n",
    "            action = possible_actions[best_action_index]\n",
    "            \n",
    "        # implement best action\n",
    "        next_state, reward, done, info = env.step(action) # reward from {0,1}, done from {True, False}\n",
    "        \n",
    "        # update reward and state\n",
    "        rewards_episode += reward\n",
    "        \n",
    "        if done:\n",
    "            next_state = np.zeros(state.shape)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            state = preprocess_state(env.reset())\n",
    "            step = max_steps\n",
    "            \n",
    "        else:\n",
    "            next_state = preprocess_state(next_state)\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "            state = next_state          \n",
    "            \n",
    "    ### train model at every episode            \n",
    "\n",
    "    # Obtain random mini-batch from memory\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    states = np.array([each[0] for each in batch]) # 6400\n",
    "    actions = np.array([each[1] for each in batch])\n",
    "    rewards = np.array([each[2] for each in batch]) \n",
    "    next_states = np.array([each[3] for each in batch])\n",
    "    dones = np.array([each[4] for each in batch])\n",
    "\n",
    "    target_Qs_batch = []\n",
    "\n",
    "    # Get maxQ values for next_state \n",
    "    if episode>=1:\n",
    "        next_state_maxQs = get_prediction(next_states, Q_classifier)['predicted_maxQ']\n",
    "    else:\n",
    "        next_state_maxQs = [0] * batch_size\n",
    "\n",
    "    # Calculate Target Q values for each state in batch\n",
    "    for i in range(0, len(batch)):\n",
    "        done = dones[i]\n",
    "        if done:\n",
    "            target_Qs_batch.append(rewards[i])\n",
    "        else:\n",
    "            target = rewards[i] + gamma * np.max(next_state_maxQs[i]) ### Bellman Equation\n",
    "            target_Qs_batch.append(target)\n",
    "\n",
    "    target_Qs = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "    # define input\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x = {'states': states,\n",
    "             'actions': np.array([np.array([1.0,0.0], dtype = np.float64) \\\n",
    "                                  if action == 2 else \\\n",
    "                                  np.array([0.0,1.0], dtype = np.float64) for action in actions])},\n",
    "        y = target_Qs, # targets/labels\n",
    "        batch_size = batch_size, \n",
    "        num_epochs = None,\n",
    "        shuffle = True)\n",
    "\n",
    "    # train model\n",
    "    Q_classifier.train(\n",
    "        input_fn = train_input_fn,\n",
    "        steps = 1,\n",
    "        hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "- https://github.com/dhruvp/atari-pong/blob/master/me_pong.py\n",
    "- https://github.com/dmlc/minpy/blob/master/examples/rl/policy_gradient/pong_model.py\n",
    "- https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "- https://github.com/gabrielgarza/openai-gym-policy-gradient/blob/master/run_carracing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](http://karpathy.github.io/assets/rl/sl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_gradient_NN(features, labels, mode):\n",
    "    \n",
    "    # During train, read discounted rewards\n",
    "    rewards  = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        rewards  = features['rewards']\n",
    "        \n",
    "    # input state\n",
    "    input_layer = features[\"states\"] # - 6400\n",
    "    \n",
    "    # hidden layer\n",
    "    num_hidden_units = 200\n",
    "    hidden = tf.layers.dense(inputs = input_layer , \n",
    "                            units = num_hidden_units,\n",
    "                            activation = tf.nn.relu)\n",
    "\n",
    "    # FC layer (dense layer)\n",
    "    logits = tf.layers.dense(inputs = hidden, units = 2)# move up or down\n",
    "\n",
    "    Final_EstimatorSpec = GenerateEstimatorSpec_PG(logits, labels, mode, rewards)\n",
    "    return(Final_EstimatorSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateEstimatorSpec_PG(logits, actions, mode, rewards):\n",
    "\n",
    "    # Generate Predictions\n",
    "    outputs_softmax = tf.nn.softmax(logits,name = \"V_2\")\n",
    "    predictions = {\n",
    "      \"best_action_index\":  tf.argmax(input = logits, axis = 1,name = \"V_1\"),\n",
    "      \"predicted_softmax\": outputs_softmax\n",
    "    }\n",
    "    \n",
    "    # If during PREDICTION mode, just return the predictions\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, \n",
    "                                          predictions = predictions)\n",
    "    \n",
    "    # If during TRAIN, calculate weighted loss\n",
    "    log_prob = tf.log(outputs_softmax) # take log, shape is (batch_size, 2)\n",
    "    indices = tf.range(0, tf.shape(log_prob)[0]) * \\\n",
    "              tf.shape(log_prob)[1] + actions\n",
    "    act_prob = tf.gather(tf.reshape(log_prob, [-1]), indices)\n",
    "    loss = -tf.reduce_sum(tf.multiply(act_prob, rewards))\n",
    "\n",
    "    # If during TRAIN, update gradients\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step = tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, \n",
    "                                          loss = loss, \n",
    "                                          train_op = train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_and_norm(episode_rewards, gamma):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0\n",
    "    for t in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[t]\n",
    "        discounted_episode_rewards[t] = cumulative\n",
    "\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './model_files_4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c20a57b70>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "PG_classifier = tf.estimator.Estimator(\n",
    "    model_fn = policy_gradient_NN, # model function type, \n",
    "    model_dir = \"./model_files_4\",\n",
    "    params = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(states, PG_classifier):\n",
    "    pred_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x = {'states': states}, #features \n",
    "        y = None,\n",
    "        shuffle = False)\n",
    "    \n",
    "    best_action_index = []\n",
    "    pred_results = PG_classifier.predict(input_fn = pred_input_fn)\n",
    "    for pred in pred_results:\n",
    "        (p_0, p_1) = pred['predicted_softmax']\n",
    "        if np.random.uniform() > p_0:\n",
    "            best_action_index.append(1)\n",
    "        else:\n",
    "            best_action_index.append(0)\n",
    "    return best_action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ytimg.com/vi/TFxhMDiTmDI/maxresdefault.jpg\" width=\"600\">\n",
    "\n",
    "<img src=\"https://leimao.github.io/images/articles/2017-05-04-REINFORCE-Policy-Gradient/Sutton_REINFORCE.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic\n",
    "total_episodes = 5       # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 2000            # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Memory\n",
    "memory_size = 50000\n",
    "batch_size = 10000\n",
    "pretrain_length = batch_size\n",
    "possible_actions = [2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class records:\n",
    "    def __init__(self):\n",
    "        self.records = {'states':[], 'actions':[], 'rewards':[]}\n",
    "    \n",
    "    def add(self, state, action, reward):\n",
    "        self.records['states'].append(state)\n",
    "        self.records['actions'].append(action)\n",
    "        self.records['rewards'].append(reward)\n",
    "        \n",
    "    def get(self, attribute):\n",
    "        return np.array(self.records[attribute], np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "tensors_to_log = {\"best_action_index\": \"V_1\", \"output_soft_max\":\"V_2\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors = tensors_to_log, every_n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    \n",
    "    # initialize game\n",
    "    state = preprocess_state(env.reset())\n",
    "    records_episode = records()\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    while step < max_steps:\n",
    "        \n",
    "        # increase step\n",
    "        env.render()\n",
    "        step += 1\n",
    "        \n",
    "        # decide action\n",
    "        if (episode == 0): # exploration\n",
    "            action = random.choice(possible_actions) # action sampled from {2,3}\n",
    "        else:\n",
    "            action = possible_actions[get_prediction(np.array([state], np.float64), #(1,6400)\n",
    "                                                     PG_classifier)[0]]\n",
    "            \n",
    "        # implement best action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        records_episode.add(state, action, reward)\n",
    "        \n",
    "        if done:\n",
    "            state = preprocess_state(env.reset())\n",
    "            step = max_steps\n",
    "            \n",
    "        else:\n",
    "            next_state = preprocess_state(next_state)\n",
    "            state = next_state \n",
    "            \n",
    "    ### train model at every episode            \n",
    "    discount_and_norm_rewards = discount_and_norm(records_episode.get('rewards'), gamma)\n",
    "    \n",
    "    # define input\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        \n",
    "        x = {'states': records_episode.get('states'),\n",
    "             'rewards': discount_and_norm_rewards},\n",
    "        \n",
    "        y = np.array([0 if action == 2 else 1 for action in records_episode.get('actions')], \n",
    "                     np.int32), \n",
    "        batch_size = batch_size, \n",
    "        num_epochs = None,\n",
    "        shuffle = True)\n",
    "\n",
    "    # train model\n",
    "    PG_classifier.train(\n",
    "        input_fn = train_input_fn,\n",
    "        steps = 1,\n",
    "        hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha-Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0112.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
