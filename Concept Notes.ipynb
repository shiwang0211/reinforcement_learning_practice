{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Multi-Bandit and Exploration/Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For each time step $t$:\n",
    "- **$\\epsilon$ Greedy Algorithm**:\n",
    "    - if $rand() < \\epsilon$ random select $k$ from $1$ to $K$\n",
    "    - if $rand() > \\epsilon$, $k = \\underset{a}{argmax\\ }(Q(a))$\n",
    "    - Usually $\\epsilon = \\frac{1}{\\sqrt {t}}$\n",
    "- Take action $k$, record reward $r$\n",
    "- Update action times\n",
    "$$N(k) \\rightarrow N(k) + 1$$\n",
    "- Update action values:\n",
    "$$Q(k) \\rightarrow Q(k) + \\frac{1}{N(k)}[r - Q(k)]$$\n",
    "- Interpretation:\n",
    "$$NewEstimate \\rightarrow OldEstimate + StepSize[Target - OldEstimate]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Property of a state\n",
    "- $P(S_{t+1}|S_t) = P(S_{t+1}|S_t, S_{t-1}, S_{t-2}, ...)$, i.e, given the present, future is independent of past states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Process (MP) - <span style=\"color:red\"> ***(S,P)***</span>\n",
    "- A sequence of random states $S$ with *Markoe property*, with the defintions of:\n",
    "    - $S$: a finite set of states\n",
    "    - $P$: transition function - $P(S_{t+1} = s'|S_t=s)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process - <span style=\"color:red\"> ***(S,P, R, $\\gamma$)***</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward Function**:\n",
    "$$R(s) = R(S_t=s) = E[r_{t+1}|S_t=s]$$\n",
    "\n",
    "**Return Function**: $$G_t =  r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "**Value Function & Bellman Expectation Equation**\n",
    "\n",
    "- how good is it to be in a particular state\n",
    "$$V_{\\pi}(s) = E[G_t|S_t=s]$$\n",
    "\n",
    "- Two components:\n",
    "    - <span style=\"color:red\"> Immediate reward</span> \n",
    "    - <span style=\"color:blue\"> Discounted value of the successor state</span> \n",
    "\n",
    "- Interpretation:\n",
    "    -  The expected reward we get upon leaving that state\n",
    "    \n",
    "    \n",
    "$$V(s) = E[G_t|S_t=s] = \\\\ E[\\color{red}{r_{t+1}} +  \\color{blue}{\\gamma(r_{t+2} + \\gamma r_{t+3} +...)}|S_t=s] = \\\\ E[\\color{red}{r_{t+1}} +  \\color{blue}{\\gamma G_{t+1}}|S_t=s] = \\\\ E[\\color{red}{r_{t+1}} +  \\color{blue}{\\gamma V(S_{t+1}})|S_t=s] = \\\\\n",
    "\\color{red}{R(s)} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}V(s')}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical solution to Bellman Equation**\n",
    "- Computational complexity: $O(n^3)$\n",
    "$$\\mathbf V = \\mathbf R + \\gamma \\mathbf P \\mathbf V$$\n",
    "$$\\mathbf V  = (\\mathbf I - \\gamma \\mathbf P)^{-1} \\mathbf R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process - <span style=\"color:red\"> ***(S, A, P, R, $\\gamma$)***</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New State Transition Probability - with action $a$**\n",
    "\n",
    "$$P_{ss'}^a = P[S_{t+1} = s'|S_t = s, A_t = a]$$\n",
    "\n",
    "\n",
    "**New Reward Function - with action $a$**\n",
    "$$R_s^a = R(S_t=s, A_t = a) = E[r_{t+1}|S_t=s, A_t = a]$$\n",
    "\n",
    "\n",
    "**Policy**:\n",
    "$$\\pi (a|s) = P(A_t=a|S_t=s)$$\n",
    "- Given a fixed policy, the **MDP** becomes **MRP**.\n",
    "\n",
    "\n",
    "**New State Transition Probability - with policy $\\pi$**\n",
    "\n",
    "$$P_{ss'}^\\pi = \\sum_{a \\in A} \\pi (a|s) P_{ss'}^a$$\n",
    "\n",
    "\n",
    "**New Reward Function - with policy $\\pi$**\n",
    "\n",
    "$$R_s^\\pi = \\sum_{a \\in A} \\pi (a|s) R_s^a$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Value Function & Bellman Expectation Equation - with policy $\\pi$**\n",
    "\n",
    "$$V_{\\pi}(s) = E_{\\pi}[\\color{red}{r_{t+1}} +  \\color{blue}{\\gamma V_{\\pi}(S_{t+1}})|S_t=s] = $$\n",
    "\n",
    "\n",
    "\n",
    "**New Action-Value Function - with policy $\\pi$ and action $a$**:\n",
    "\n",
    "- how good is it to be in a particular state with a particular action\n",
    "$$q_{\\pi}(s;a) = E_{\\pi}[G_t|S_t=s;A_t=a]= \\\\\n",
    "E_{\\pi}[\\color{red}{r_{t+1}} +  \\color{blue}{\\gamma q_{\\pi}(S_{t+1}, A_{t+1})}|S_t=s, A_t = a]$$  \n",
    "\n",
    "**The relationship between $V$ and $q$**\n",
    "$$V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) q_{\\pi}(s,a) --\\ [1]$$\n",
    "\n",
    "**Rewrite New Action-Value Function**\n",
    "\n",
    "$$q_{\\pi}(s, a) =\n",
    "E_{\\pi}[\\color{red}{r_{t+1}} +  \\color{blue}{\\gamma q_{\\pi}(S_{t+1}, A_{t+1})}|S_t=s, A_t = a] =\\\\\n",
    "\\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^a {\\sum_{a' \\in \\mathbf A}} \\pi(a'|s') q_{\\pi}(s', a')}} \\xrightarrow{\\text{[1]}} \\\\\n",
    "\\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV_{\\pi}(s')}} --- [3]$$ \n",
    "\n",
    "**Rewrite New Value Function**\n",
    "$$V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)[\\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV_{\\pi}(s')}}] ---\\ [2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical solution to New Bellman Equation**\n",
    "$$\\mathbf V_{\\pi}  = \\mathbf R^{\\pi} + \\gamma \\mathbf P^{\\pi}  \\mathbf V$$\n",
    "$$\\mathbf V_{\\pi}   = (\\mathbf I - \\gamma \\mathbf P^{\\pi} )^{-1} \\mathbf R^{\\pi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bellman Optimality Equation**\n",
    "- Optimal Policy\n",
    "$$\\pi^*(s) = \\underset{\\pi}{argmax\\ }V_{\\pi}(s)$$\n",
    "- Optimal Value Function\n",
    "$$V^*(s) = \\underset{\\pi}{max\\ }V_{\\pi}(s)$$\n",
    "\n",
    "- Optimal Value-Action Function\n",
    "$$q^*(s,a) = \\underset{\\pi}{max\\ }q_{\\pi}(s,a)$$\n",
    "\n",
    "**Relationship between optimal functions**\n",
    "$$V^*(s) =  \\underset{a}{max\\ }q^*(s,a)$$\n",
    "$$q^*(s,a) = \\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV^{*}(s')}}$$\n",
    "\n",
    "**Rewrite Optimal Value Function**\n",
    "$$V^*(s) =  \\underset{a}{max\\ }[\\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV^{*}(s')}}]$$\n",
    "\n",
    "$$q^*(s,a) = \\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^a }\\underset{a'}{max\\ }{q^*(s',a')}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based Approach (known $P$ and $r$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "- Given MDP and policy $\\pi$\n",
    "- Using analytical solution to Bellman Equation\n",
    "**Rewrite New Value Function**\n",
    "$$V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s)[\\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV_{\\pi}(s')}}] ---\\ [2]$$\n",
    "$$\\mathbf V  = \\mathbf R^{\\pi} + \\gamma \\mathbf P^{\\pi}  \\mathbf V$$\n",
    "\n",
    "- Iteration from $k$ to $k+1$:\n",
    "\n",
    "$$\\mathbf V^{k+1}  = \\mathbf R^{\\pi} + \\gamma \\mathbf P^{\\pi}  \\mathbf V^k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "- **Policy Evaluation**: Given $\\pi_i$, evalute $V_{\\pi_i}$ for each state $s$.\n",
    "\n",
    "- **Policy Improvement**: for each state $s$:\n",
    "    - Calculate $q(s,a)$ for each $a$\n",
    "$$q_{\\pi}(s, a) =\n",
    "\\color{red}{R_s^a} + \\color{blue}{\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV_{\\pi}(s')}} --- [3]$$ \n",
    "    - Get updated policy $\\pi_{i+1}(s)$\n",
    "$$\\pi_{i+1}(s) = greedy\\ (V_{\\pi_i}(s)) = \\underset{a}{argmax\\ }\\ q_{\\pi_i}(s,a)$$\n",
    "\n",
    "- **Convergence**: can be mathematically improved\n",
    "- **Stop Criteria**:\n",
    "    - Assume current policy $\\pi_i$ is already optimal\n",
    "$$q_{\\pi_i}(s, \\pi_{i+1}(s)) = \\xrightarrow{\\text{policy improvement}}\\underset{a}{max\\ }\\ q_{\\pi_i}(s,a) =\\xrightarrow{\\text{already optimal}} q_{\\pi_i}(s,\\pi_i(s))=\\xrightarrow{\\text{[1]}}V_{\\pi_i}(s)$$\n",
    "    - Then $\\pi_i$ is the optimal $\\pi^*$, we have:$$V_{\\pi_i}(s) = \\underset{a}{max\\ }\\ q_{\\pi_i}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "- Directly find the optimal value function of each state instead of explicitly finding the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Value Update**: \n",
    "$$V_{i+1}(s) = \\underset{a}{max\\ }[{R_s^a} + {\\gamma \\sum_{s' \\in \\mathbf S}{P_{ss'}^aV_{i}(s')}}]$$\n",
    "- **Stop Criteria**: Difference in $V$ value \n",
    "- **Optimal Policy**: Action with max $V$ for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_06.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free Approach (Unknown $P$ and $r$)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo (MC) Methods\n",
    "- Update until the return is known\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [G_t - V(S_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/pd.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient - Monte Carlo REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The parameters $\\theta$ in the $NN$ determines a policy $\\pi_{\\theta}$\n",
    "- Define trajectory $\\tau$\n",
    "$$\\tau = (a_1, s_1, a_2, s_2, ...)$$\n",
    "- Define total reward $r(\\tau)$\n",
    "$$r(\\tau) = \\sum_{t \\geq 0} \\gamma^t r_t$$\n",
    "- The loss function for a given policy $\\pi_{\\theta}$ is\n",
    "$$J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}}[r(\\tau) ]$$\n",
    "- A mathematical property can be proved\n",
    "$$\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}}[\\color{red}{\\nabla_\\theta\\ log\\pi_{\\theta}(\\tau)}\\ \\color{blue}{ r(\\tau)}] = E_{\\tau \\sim \\pi_{\\theta}}[\\color{red}{\\sum_{t=1}^T \\nabla_\\theta\\ log\\pi_{\\theta}(a_t|s_t)}\\ \\color{blue}{ r(\\tau)}] --- [5]$$ \n",
    "- There are other revisions to replace $\\color{blue}{ r(\\tau)}$ with $\\color{blue}{ \\Phi(\\tau, t)}$ to account for various problems. (e.g., standardization for all positive rewards, only calcultate the reward of $a_t$ from $t+1$, etc.). For example:\n",
    "$$\\color{blue}{ \\Phi(\\tau, t)} = \\sum_{t' \\geq t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "\n",
    "- Step 1: sample $\\tau = (a_1, s_1, a_2, s_2, ..., a_T, s_T)$ based on $\\pi_{\\theta}$, and get $r(\\tau)$\n",
    "- Step 2: Calculate back propagation for $\\nabla_\\theta J(\\theta)$. For each time step $t$, it's multiclass-classification $NN$ with target value as $\\color{blue}{r(\\tau)}$ or $\\color{blue}{ \\Phi(\\tau, t)}$.\n",
    "- Step 3: Update weiights $w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drawbacks**\n",
    "- Update after each episode instead of each time step. There may be some bad actions.\n",
    "- Require large sample size.\n",
    "<img src=\"./figure/policy_gradient.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "see this [notebook](RL%20Practice.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-Difference (TD) Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD methods need to wait only until the next time step.\n",
    "- At time t + 1 they immediately form a target and make a useful update.\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [r_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] = \\color{red}{(1-\\alpha)V(S_t)} + \\color{blue}{\\alpha [r + \\gamma V(S_{t+1})]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Key idea**: the learned action-value function $Q$ directly approximates $q^*$, the optimal action-value function\n",
    "- **Optimal Policy**:\n",
    "$$\\pi^*(s) = \\underset{a}{argmax\\ }Q(s,a) $$\n",
    "- ref: http://www.incompleteideas.net/book/RLbook2020.pdf\n",
    "\n",
    "\n",
    "- Initialize $Q$ table with $Q(s,a)$, note that $Q(terminal, \\cdot)=0$\n",
    "- Loop for each episode:\n",
    "    - Initialize S\n",
    "    - Loop for each step of episode:\n",
    "        - Choose action $a$ from S using policy derived from $Q$\n",
    "        - Take action $a$, observe reward $r$ and next state $s'$.\n",
    "        - Update $Q$ table: \n",
    "        $$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\underset{a'}{max\\ } Q(s',a') - Q(s,a)] = \\\\\n",
    "        \\color{red}{(1-\\alpha)Q(s,a)} + \\color{blue}{\\alpha [r + \\gamma \\underset{a'}{max\\ } Q(s',a')]} --- [4]$$\n",
    "        - $s \\leftarrow s'$\n",
    "    - until $s$ is terminal\n",
    "\n",
    "\n",
    "**Drawbacks**:\n",
    "- A finite set of actions. Cannot handle continuous or stochatisc action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_09.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The parameters $w$ in the $NN$ determines a policy $Q(s,a|s, w)$ for each $a$.\n",
    "<img src=\"./figure/deepq.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For episode in range(max_episode):\n",
    "    - For step in range(max_step):\n",
    "        - From $s$, run $NN$, and get $\\hat Q_w(s,a)$ for each action a.\n",
    "        - Select and take best action $a^*$\n",
    "        - Get reward $r$ and Get next state $s'$.\n",
    "        - Run $NN$ again to get $Q(s',a')$ for each $a'$\n",
    "        - If $s'$ is not terminal: Set **target value** $\\color{blue}{Q(s,a^*) = [r + \\gamma \\underset{a'}{max\\ } Q(s',a')]} --- [4]$\n",
    "        - Set loss $L= [\\hat Q_w(s,a^*) - \\color{blue}{Q(s,a^*)}] ^2$\n",
    "        - Update weights\n",
    "        $$\\nabla_w L(w) = [\\hat Q_w(s,a^*) - \\color{blue}{Q(s,a^*)}] \\nabla_w \\hat Q(s,a^*) --- [6]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "see this [notebook](RL%20Practice.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla  Actor-Critic\n",
    "- [reference](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)\n",
    "\n",
    "- Instead of using $r(\\tau)$ (which is the emprical, long-term reward based on T steps) and updating $\\theta$ after one episode, here we use $Q(s,a)$ instead (There are also other variations). This enables updating weights every step.\n",
    "\n",
    "    - Policy Gradient:\n",
    "$$\\nabla_\\theta J(\\theta) = \\color{red}{ \\nabla_\\theta\\ log\\pi_{\\theta}(a_t|s_t)}\\ \\color{blue}{ r(\\tau)} ---[5]$$ \n",
    "    - Actor:\n",
    "    $$\\nabla_\\theta J(\\theta) = \\color{red}{ \\nabla_\\theta\\ log\\pi_{\\theta}(a_t|s_t)}\\ \\color{blue}{ Q(S_t,A_t)} --- [7]$$ \n",
    "    - Deep Q-Learning\n",
    "   $$\\nabla_w L(w) = [\\hat Q_w(s,a^*) - \\color{blue}{(r + \\gamma \\underset{a'}{max\\ } Q(s',a'))}] \\nabla_w \\hat Q(s,a^*) --- [6]$$\n",
    "    - Critic\n",
    "     $$\\nabla_w L(w) = [ \\underbrace{\\hat Q_w(S_t,A_t) -\\color{blue}{(r + \\gamma  Q(S_{t+1},A_{t+1}))}}_\\color{blue}{\\text{correction (TD error)}}] \\nabla_w \\hat Q(S_t,A_t)---[8] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**\n",
    "- Intialize $s, \\theta, w$, and sample $a$ from based on $\\pi_\\theta$.\n",
    "- For $t = 1, 2, ... ,T$\n",
    "    - get $s'$ and $r$\n",
    "    - get $a'$ based on $\\pi_\\theta(a'|s')$\n",
    "    - Update $\\theta$ based on $[7]$\n",
    "    - Update $w$ based on $[8]$\n",
    "    - $a \\leftarrow a'$, $s \\leftarrow s'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "- Both REINFORCE and the vanilla version of actor-critic method are on-policy: training samples are collected according to the target policy â€” the very same policy that we try to optimize for.\n",
    "- DDPG is a model-free off-policy actor-critic algorithm\n",
    "- https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "<img src=\"./figure/ddpg.png\" width=\"700\">\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"./figure/ddpg2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with Deep Q Learning**\n",
    "<img src=\"./figure/ddpg3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "<img src=\"https://planspace.org/20170830-berkeley_deep_rl_bootcamp/img/landscape.png\" width=\"600\">\n",
    "\n",
    "\n",
    "ref: https://antkillerfarm.github.io/rl/2018/11/18/RL.html\n",
    "\n",
    "<img src=\"./figure/rl2.png\" width=\"800\">\n",
    "\n",
    "- Action space\n",
    "    - DQN cannot deal with infinite actions (e.g., driving)\n",
    "    - Policy gradient can learn stochastic policies while DQN has deterministic policy given state.\n",
    "    \n",
    "    \n",
    "- Convergence\n",
    "    - Policy gradient has better convergence (policy is updated smoothly, while in DQN, slight change in Q value may completely change action/policy space, thus no convergence guarantee)\n",
    "    - Policy gradient has guaranteed for local minimum at least\n",
    "    - But usually policy gradient takes longer to train)\n",
    "\n",
    "\n",
    "- Variance\n",
    "    - Policy gradient: Higher variance and sampling inefficiency. (Think of a lot of actions taken in a epoch before updating)\n",
    "    - Actor-Critic: TD update. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/rl4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Left: model unknown\n",
    "- Right: model known\n",
    "- Top: update each step\n",
    "- Bottom: update each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
