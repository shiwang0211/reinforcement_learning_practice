{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Definition-of-MDP\" data-toc-modified-id=\"Definition-of-MDP-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Definition of MDP</a></span></li><li><span><a href=\"#Known-MDP-(given-T,-R)\" data-toc-modified-id=\"Known-MDP-(given-T,-R)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Known MDP (given T, R)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Value-Iteration\" data-toc-modified-id=\"Value-Iteration-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Value Iteration</a></span></li><li><span><a href=\"#Policy-Iteration\" data-toc-modified-id=\"Policy-Iteration-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Policy Iteration</a></span></li></ul></li><li><span><a href=\"#Unknown-MDP---Model-Free\" data-toc-modified-id=\"Unknown-MDP---Model-Free-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Unknown MDP - Model Free</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q-Learning\" data-toc-modified-id=\"Q-Learning-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Q-Learning</a></span></li><li><span><a href=\"#Deep-Q-Learning\" data-toc-modified-id=\"Deep-Q-Learning-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Deep Q Learning</a></span></li><li><span><a href=\"#Policy-Gradient\" data-toc-modified-id=\"Policy-Gradient-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Policy Gradient</a></span></li><li><span><a href=\"#Actor-Critic\" data-toc-modified-id=\"Actor-Critic-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Actor-Critic</a></span></li><li><span><a href=\"#Comparison\" data-toc-modified-id=\"Comparison-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Comparison</a></span></li></ul></li><li><span><a href=\"#MCTS-(Monte-Carlo-Tree-Search)\" data-toc-modified-id=\"MCTS-(Monte-Carlo-Tree-Search)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>MCTS (Monte-Carlo Tree Search)</a></span></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of MDP\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_03.png\" width=\"700\">\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_04.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known MDP (given T, R) \n",
    "## Value Iteration\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_06.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown MDP - Model Free\n",
    "## Q-Learning\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_08.png\" width=\"700\">\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_09.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UIAk_UoTaLMEGilFNamBlQ.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_13.png\" width=\"700\">\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "\n",
    "Main differen with policy gradient:\n",
    "- Instead of using $r(\\tau)$ (which is the emprical, long-term reward based on T steps) and after one episode and update $\\theta$, use $Q(s,a)$ instead. \n",
    "\n",
    "- This enables updating weights every step.\n",
    "- To Read: https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d\n",
    "\n",
    "<img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_model_free_active_actor_critic_neural_implementation_outcome.png\" width=\"700\">\n",
    "\n",
    "<img src=\"https://planspace.org/20170830-berkeley_deep_rl_bootcamp/img/landscape.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "1. Action space\n",
    "    - DQN cannot deal with infinite actions (e.g., driving)\n",
    "    - Policy gradient can learn stochastic policies (while DQN has deterministic policy given state)\n",
    "    \n",
    "    \n",
    "2. Convergence\n",
    "    - Policy gradient has better convergence (policy is updated smoothly, while in DQN, slight change in Q value may completely change action/policy space, thus no convergence guarantee)\n",
    "    - Policy gradient has guaranteed for local minimum at least\n",
    "    - But usually policy gradient takes longer to train)\n",
    "\n",
    "\n",
    "3. Variance\n",
    "    - Policy gradient: Higher variance and sampling inefficiency. (Think of a lot of actions taken in a epoch before updating)\n",
    "    - Actor-Critic: TD update. See explanation: https://www.quora.com/Whats-the-difference-between-Reinforce-and-Actor-Critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS (Monte-Carlo Tree Search)\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_15.png\" width=\"700\">\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_16.png\" width=\"700\">\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf\n",
    "- https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
