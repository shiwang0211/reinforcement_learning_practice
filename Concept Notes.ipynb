{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Multi-Bandit and Exploration/Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$ Greedy Algorithm:\n",
    "\n",
    "For each time step $t$:\n",
    "- if $rand() < \\epsilon$ random select $k$ from 1 to $K$\n",
    "- if $rand() > \\epsilon$, $k = argmax_i(Q(i))$\n",
    "- Take action $k$, record reward $r$\n",
    "- Update $Q(k) = \\frac {Q_k * M + r}{M + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of MDP\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_03.png\" width=\"700\">\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_04.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known MDP (given T, R) \n",
    "## Value Iteration\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_06.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown MDP - Model Free\n",
    "## Q-Learning\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_08.png\" width=\"700\">\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_09.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UIAk_UoTaLMEGilFNamBlQ.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_13.png\" width=\"700\">\n",
    "\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "\n",
    "Main differen with policy gradient:\n",
    "- Instead of using $r(\\tau)$ (which is the emprical, long-term reward based on T steps) and after one episode and update $\\theta$, use $Q(s,a)$ instead. \n",
    "\n",
    "- This enables updating weights every step.\n",
    "- To Read: https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d\n",
    "\n",
    "<img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_model_free_active_actor_critic_neural_implementation_outcome.png\" width=\"700\">\n",
    "\n",
    "<img src=\"https://planspace.org/20170830-berkeley_deep_rl_bootcamp/img/landscape.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "1. Action space\n",
    "    - DQN cannot deal with infinite actions (e.g., driving)\n",
    "    - Policy gradient can learn stochastic policies (while DQN has deterministic policy given state)\n",
    "    \n",
    "    \n",
    "2. Convergence\n",
    "    - Policy gradient has better convergence (policy is updated smoothly, while in DQN, slight change in Q value may completely change action/policy space, thus no convergence guarantee)\n",
    "    - Policy gradient has guaranteed for local minimum at least\n",
    "    - But usually policy gradient takes longer to train)\n",
    "\n",
    "\n",
    "3. Variance\n",
    "    - Policy gradient: Higher variance and sampling inefficiency. (Think of a lot of actions taken in a epoch before updating)\n",
    "    - Actor-Critic: TD update. See explanation: https://www.quora.com/Whats-the-difference-between-Reinforce-and-Actor-Critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS (Monte-Carlo Tree Search)\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_15.png\" width=\"700\">\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_16.png\" width=\"700\">\n",
    "<img src=\"./notes/Reinforcement_Learning_Page_17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf\n",
    "- https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
